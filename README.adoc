= A Kafka Broker Cluster on Multiple VMs +
Some text needs to be here, I guess but this is bunk +
. +
== Build a Single Kafka Node VM
. create directory for new cluster “ kafka-ubuntu-cluster-node-1”
. change directory to “kafka-ubuntu-cluster-node-1"
. let's init a vagrant box 'paulreese/kafka-ubuntu-cluster-node-n'
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant init paulreese/kafka-ubuntu-cluster-node-n
---------------------------------------------------------------------
+
. edit the Vagrantfile;
.. uncomment "config.vm.network and set the private IP address; you can use 192.168.33.21 (for node-1)
+
[source, numbered]
---------------------------------------------------------------------
# Create a private network, which allows host-only access to the machine
# using a specific IP.
config.vm.network "private_network", ip: "192.168.33.21"
---------------------------------------------------------------------
+
.. uncomment "config.vm.provider" section and set vb.memory to 2048 (leave vb.gui commented out)
+
[source, numbered]
---------------------------------------------------------------------
config.vm.provider "virtualbox" do |vb|
#   # Display the VirtualBox GUI when booting the machine
#   vb.gui = true
#
#   # Customize the amount of memory on the VM:
  vb.memory = "2048"
end 
---------------------------------------------------------------------
+
.. add config.ssh.insert_key = false, towards the end of the file (just before "end"). this is to partly handle vagrant user's SSH key
+
[source, numbered]
---------------------------------------------------------------------
config.ssh.insert_key = false
---------------------------------------------------------------------
+
.. save and exit Vagrantfile
. start up the new box and ssh into VM;
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant up --provider virtualbox
---------------------------------------------------------------------
+
. open a terminal 
. cd to directory "kafka-ubuntu-cluster-node-4"
. ssh into vagrant box, while it is being built - password = vagrant
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant ssh
vagrant@127.0.0.1's password:
---------------------------------------------------------------------
+
. edit `~vagrant/.ssh/authorized_keys`
. remove existing ssh-rsa key entry
. add this ssh-rsa entry "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key
"
. should now look like this
+
[source, numbered]
---------------------------------------------------------------------
# CLOUD_IMG: This file was created/modified by the Cloud Image build process
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key
---------------------------------------------------------------------
+
. save and exit
. on the VM, the Kafka server settings should be set. You can verify the settings;
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/server.properties
---------------------------------------------------------------------
+
. add these properties to the server.properties
+
[source, numbered]
---------------------------------------------------------------------
broker.id=1
listeners=PLAINTEXT://192.168.33.21:9092
host.name=192.168.33.21
advertised.host.name=192.168.33.21
---------------------------------------------------------------------
+
. save and exit file
. zookeeper settings should be set. You can verify the settings;
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/zookeeper.properties
---------------------------------------------------------------------
+
.  leave these as is (unless you are using another host IP address), we will update later once we have three nodes running. For quorum we need odd number of Zookeeper nodes;
+
[source, numbered]
---------------------------------------------------------------------
server.1=192.168.33.21:2888:3888
#server.1=192.168.33.22:2888:3888
#server.1=192.168.33.23:2888:3888
---------------------------------------------------------------------
+
. save and exit file
. add a myid file to store the Kafka broker id (as set above);
+
[source, numbered]
---------------------------------------------------------------------
$ echo 1 > /var/lib/zookeeper/myid
---------------------------------------------------------------------
+
. restart Zookeeper server
+
[source, numbered]
---------------------------------------------------------------------
$ sudo /etc/init.d/zookeeper stop
$ sudo /etc/init.d/zookeeper start
---------------------------------------------------------------------
+
. restart Kafka server
+
[source, numbered]
---------------------------------------------------------------------
$ sudo /etc/init.d/kafka-server stop
$ sudo /etc/init.d/kafka-server start
---------------------------------------------------------------------
+
. create a new topic “here”
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-topics --create --zookeeper 192.168.33.21:2181 --partition 1 --replication-factor 1 --topic there
---------------------------------------------------------------------
+
. produce messages to “here” topic from the console;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-producer --broker-list 192.168.33.21:9092 --topic here
---------------------------------------------------------------------
+
. enter some text in console;
+
[source, numbered]
---------------------------------------------------------------------
this is
node
one
---------------------------------------------------------------------
+
. consume messages from “here” topic;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-consumer --zookeeper 192.168.33.21:2181 --topic here
---------------------------------------------------------------------
+
. we’ve now successfully built out a single Zookeeper node and a single Kafka broker

== Add Kafka Broker Node 2
. create directory for new cluster “kafka-ubuntu-cluster-node-2”
. change directory to “kafka-ubuntu-cluster-node-2"
. let's init a vagrant box 'paulreese/kafka-ubuntu-cluster-node-n'
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant init paulreese/kafka-ubuntu-cluster-node-n
---------------------------------------------------------------------
+
. edit the Vagrantfile;
.. uncomment "config.vm.network and set the private IP address; you can use 192.168.33.22 (for node-2)
+
[source, numbered]
---------------------------------------------------------------------
# Create a private network, which allows host-only access to the machine
# using a specific IP.
config.vm.network "private_network", ip: "192.168.33.22"
---------------------------------------------------------------------
+
.. uncomment "config.vm.provider" section and set vb.memory to 2048 (leave vb.gui commented out)
+
[source, numbered]
---------------------------------------------------------------------
config.vm.provider "virtualbox" do |vb|
#   # Display the VirtualBox GUI when booting the machine
#   vb.gui = true
#
#   # Customize the amount of memory on the VM:
  vb.memory = "2048"
end 
---------------------------------------------------------------------
+
.. add config.ssh.insert_key = false, towards the end of the file (just before "end"). this is to partly handle vagrant user's SSH key
+
[source, numbered]
---------------------------------------------------------------------
config.ssh.insert_key = false
---------------------------------------------------------------------
+
.. save and exit Vagrantfile
. start up the new box and ssh into VM;
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant up --provider virtualbox
---------------------------------------------------------------------
+
. open a terminal 
. cd to directory "kafka-ubuntu-cluster-node-4"
. ssh into vagrant box, while it is being built - password = vagrant
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant ssh
vagrant@127.0.0.1's password:
---------------------------------------------------------------------
+
. edit `~vagrant/.ssh/authorized_keys`
. remove existing ssh-rsa key entry
. add this ssh-rsa entry "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key
"
. should now look like this
+
[source, numbered]
---------------------------------------------------------------------
# CLOUD_IMG: This file was created/modified by the Cloud Image build process
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key
---------------------------------------------------------------------
+
. save and exit
. on the VM, the Kafka server settings should be set. You can verify the settings;
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/server.properties
---------------------------------------------------------------------
+
. set these Kafka settings
+
[source, numbered]
---------------------------------------------------------------------
broker.id=2
listeners=PLAINTEXT://192.168.33.22:9092
host.name=192.168.33.22
advertised.host.name=192.168.33.22
---------------------------------------------------------------------
+
. save and exit file

. add a myid file to store the Kafka broker id (as set above);
+
[source, numbered]
---------------------------------------------------------------------
$ echo 2 > /var/lib/zookeeper/myid
---------------------------------------------------------------------
+
. restart Kafka server
+
[source, numbered]
---------------------------------------------------------------------
$ sudo /etc/init.d/kafka-server stop

$ sudo /etc/init.d/kafka-server start
---------------------------------------------------------------------
+
. On “kafka-ubuntu-cluster-node-2” produce messages to “here” topic from the console;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-producer --broker-list 192.168.33.22:9092 --topic here
---------------------------------------------------------------------
+
.. enter some text in console;
+
[source, numbered]
---------------------------------------------------------------------
this is
node
two
---------------------------------------------------------------------
+
. On “kafka-ubuntu-cluster-node-1” consume messages from “here” topic;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-consumer --zookeeper 192.168.33.21:2181 --topic here
---------------------------------------------------------------------
+
.

== Add Kafka Broker Node 3
. create directory for new cluster “kafka-ubuntu-cluster-node-3”
. change directory to “kafka-ubuntu-cluster-node-3"
. let's init a vagrant box 'paulreese/kafka-ubuntu-cluster-node-n'
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant init paulreese/kafka-ubuntu-cluster-node-n
---------------------------------------------------------------------
+
. edit the Vagrantfile;
.. uncomment "config.vm.network and set the private IP address; you can use 192.168.33.23 (for node-3)
+
[source, numbered]
---------------------------------------------------------------------
# Create a private network, which allows host-only access to the machine
# using a specific IP.
config.vm.network "private_network", ip: "192.168.33.23"
---------------------------------------------------------------------
+
.. uncomment "config.vm.provider" section and set vb.memory to 2048 (leave vb.gui commented out)
+
[source, numbered]
---------------------------------------------------------------------
config.vm.provider "virtualbox" do |vb|
#   # Display the VirtualBox GUI when booting the machine
#   vb.gui = true
#
#   # Customize the amount of memory on the VM:
  vb.memory = "2048"
end 
---------------------------------------------------------------------
+
.. add config.ssh.insert_key = false, towards the end of the file (just before "end"). this is to partly handle vagrant user's SSH key
+
[source, numbered]
---------------------------------------------------------------------
config.ssh.insert_key = false
---------------------------------------------------------------------
+
.. save and exit Vagrantfile
. start up the new box and ssh into VM;
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant up --provider virtualbox
---------------------------------------------------------------------
+
. open a terminal 
. cd to directory "kafka-ubuntu-cluster-node-4"
. ssh into vagrant box, while it is being built - password = vagrant
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant ssh
vagrant@127.0.0.1's password:
---------------------------------------------------------------------
+
. edit `~vagrant/.ssh/authorized_keys`
. remove existing ssh-rsa key entry
. add this ssh-rsa entry "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key
"
. should now look like this
+
[source, numbered]
---------------------------------------------------------------------
# CLOUD_IMG: This file was created/modified by the Cloud Image build process
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key
---------------------------------------------------------------------
+
. save and exit
. on the VM, the Kafka server settings should be set. You can verify the settings;
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/server.properties
---------------------------------------------------------------------
+
. modify the file accordingly
+
[source, numbered]
---------------------------------------------------------------------
broker.id=3
listeners=PLAINTEXT://192.168.33.23:9092
host.name=192.168.33.23
advertised.host.name=192.168.33.23
---------------------------------------------------------------------
+
. save and exit file
. add a myid file to store the Kafka broker id (as set above);
+
[source, numbered]
---------------------------------------------------------------------
$ echo 3 > /var/lib/zookeeper/myid
---------------------------------------------------------------------
+
. restart Kafka server
+
[source, numbered]
---------------------------------------------------------------------
$ sudo /etc/init.d/kafka-server stop

$ sudo /etc/init.d/kafka-server start
---------------------------------------------------------------------
+

. on “kafka-ubuntu-cluster-node-3” produce messages to “here” topic from the console;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-producer --broker-list 192.168.33.21:9092,192.168.33.22:9092 --topic here
---------------------------------------------------------------------
+
.. enter some text in console;
+
[source, numbered]
---------------------------------------------------------------------
this is
node
two
---------------------------------------------------------------------
+
. on “kafka-ubuntu-cluster-node-1” consume messages from “here” topic;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-consumer --zookeeper 192.168.33.21:2181 --topic here —from-beginning
---------------------------------------------------------------------
+
.

== Let’s Setup the Zookeeper Cluster
. since we now have 3 nodes, we can setup a 3-node Zookeeper cluster
. on “kafka-ubuntu-cluster-node-1”
.. edit Kafka Server property file, we will update zookeeper.connect property on each node
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/server.properties
---------------------------------------------------------------------
+
.. edit the Kafka connect property (to Zookeeper) as follows 
+
[source, numbered]
---------------------------------------------------------------------
zookeeper.connect=192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181
---------------------------------------------------------------------
+
.. save and exit file
.. Edit Zookeeper’s property file, we will add server.x=ipaddress:port:port for each extra node in the cluster
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/zookeeper.properties
---------------------------------------------------------------------
+
.. uncomment lines
.. server.2 and server.3, should now look like this;
+
[source, numbered]
---------------------------------------------------------------------
server.1=192.168.33.21:2888:3888
server.2=192.168.33.22:2888:3888
server.3=192.168.33.23:2888:3888
---------------------------------------------------------------------
+
.. add initLimit (Amount of time, in ticks (see tickTime), to allow followers to connect and sync to a leader. Increased this value as needed, if the amount of data managed by ZooKeeper is large.) and syncLimit (Amount of time, in ticks (see tickTime), to allow followers to sync with ZooKeeper. If followers fall too far behind a leader, they will be dropped.)
+
[source, numbered]
---------------------------------------------------------------------
initLimit=5
syncLimit=2
---------------------------------------------------------------------
+
.. save and exit file
. on “kafka-ubuntu-cluster-node-2”
.. edit Kafka Server property file, we will update zookeeper.connect property on each node
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/server.properties`
---------------------------------------------------------------------
+
.. edit the Kafka connect property (to Zookeeper) as follows
+
[source, numbered]
---------------------------------------------------------------------
zookeeper.connect=192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181
---------------------------------------------------------------------
+
.. edit Zookeeper’s property file, we will add server.x=ipaddress:port:port for each extra node in the cluster
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/zookeeper.properties
---------------------------------------------------------------------
+
.. uncomment lines "server.2" and "server.3", should now look like this;
+
[source, numbered]
---------------------------------------------------------------------
server.1=192.168.33.21:2888:3888
server.2=192.168.33.22:2888:3888
server.3=192.168.33.23:2888:3888
---------------------------------------------------------------------
+
... save and exit file
. on “kafka-ubuntu-cluster-node-3”
..  Edit Kafka Server property file, we will update zookeeper.connect property on each node
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/server.properties
---------------------------------------------------------------------
+
.. edit the Kafka connect property (to Zookeeper) as follows
+
[source, numbered]
---------------------------------------------------------------------
zookeeper.connect=192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181
---------------------------------------------------------------------
+
.. edit Zookeeper’s property file, we will add server.x=ipaddress:port:port for each extra node in the cluster
+
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /etc/kafka/zookeeper.properties
---------------------------------------------------------------------
+
... uncomment lines
... server.2 and server.3, should now look like this;
+
[source, numbered]
---------------------------------------------------------------------
server.1=192.168.33.21:2888:3888
server.2=192.168.33.22:2888:3888
server.3=192.168.33.23:2888:3888
---------------------------------------------------------------------
+
... save and exit file
. Let’s halt each VM one at a time
.. on “kafka-ubuntu-cluster-node-1”
+
[source, numbered]
---------------------------------------------------------------------
$ exit

$ vagrant halt
---------------------------------------------------------------------
+
.. on “kafka-ubuntu-cluster-node-2”
+
[source, numbered]
---------------------------------------------------------------------
$ exit

$ vagrant halt
---------------------------------------------------------------------
+
.. on “kafka-ubuntu-cluster-node-2”
+
[source, numbered]
---------------------------------------------------------------------
$ exit

$ vagrant halt
---------------------------------------------------------------------
+
. Let’s start each VM one at a time
.. on “kafka-ubuntu-cluster-node-1”
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant up

$ vagrant ssh
---------------------------------------------------------------------
+
.. on “kafka-ubuntu-cluster-node-2”
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant up

$ vagrant ssh
---------------------------------------------------------------------
+
.. on “kafka-ubuntu-cluster-node-2”
+
[source, numbered]
---------------------------------------------------------------------
$ vagrant up
$ vagrant ssh
---------------------------------------------------------------------
+
. Zookeeper and Kafka servers will start via /etc/init.d scripts and in order. You can validate startup is successful by checking logs;
.. for Zookeeper we have /var/log/kafka/zookeeper.out
.. for Kafka Server we have /var/log/kafka/kafka-server.out
. on “kafka-ubuntu-cluster-node-3” produce messages to “here” topic from the console;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-producer --broker-list 192.168.33.21:9092,192.168.33.22:9092 --topic here
---------------------------------------------------------------------
+
.. enter some text in console;
+
[source, numbered]
---------------------------------------------------------------------
this is
node
two
---------------------------------------------------------------------
+
. on “kafka-ubuntu-cluster-node-1” consume messages from “here” topic;
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-consumer --zookeeper 192.168.33.21:2181 --topic here —from-beginning
---------------------------------------------------------------------
+
. let’s test again with the kafka-console-producer, this time using the third node as the broker
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-console-producer --broker-list 192.168.33.21:9092 --topic here
---------------------------------------------------------------------
+
. let’s delete topic “here”, we now have 3 replicas
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-topics --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --delete --topic here
---------------------------------------------------------------------
+
. let’s create topic “here” again
+
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-topics --create --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --partition 1 --replication-factor 3 --topic here
---------------------------------------------------------------------
+
. let’s describe the create topic “here”; note that we can see that the replicas have changed and a leader was elected
[source, numbered]
---------------------------------------------------------------------
$ /usr/bin/kafka-topics --describe --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --topic here
---------------------------------------------------------------------
= FAQ
== Configured broker.id 3 doesn't match stored broker.id 1 in meta.properties
[source, numbered]
---------------------------------------------------------------------
[2016-10-05 15:48:05,841] FATAL Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
kafka.common.InconsistentBrokerIdException: Configured broker.id 3 doesn't match stored broker.id 1 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).
        at kafka.server.KafkaServer.getBrokerId(KafkaServer.scala:648)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:187)
        at io.confluent.support.metrics.SupportedServerStartable.startup(SupportedServerStartable.java:100)
        at io.confluent.support.metrics.SupportedKafka.main(SupportedKafka.java:49)
---------------------------------------------------------------------
. update kafka meta.properties
[source, numbered]
---------------------------------------------------------------------
$ sudo vi /var/lib/kafka/meta.properties
---------------------------------------------------------------------
. set the broker's id correctly 
[source, numbered]
---------------------------------------------------------------------
set broker.id=3
---------------------------------------------------------------------
. restart kafka server
[source, numbered]
---------------------------------------------------------------------
$ sudo /etc/init.d/kafka-server start
---------------------------------------------------------------------

== To delete a topic
[source, numbered]
---------------------------------------------------------------------
/var/log/kafka$ /usr/bin/kafka-topics --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --delete --topic here

vagrant@vagrant-ubuntu-trusty-64:/var/log/kafka$ /usr/bin/kafka-topics --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --list
__confluent.support.metrics
__consumer_offsets
_schemas
greet
here . marked for deletion
test
there . marked for deletion
---------------------------------------------------------------------

== If you see topic is “marked for deletion”, add this to /etc/kafka/server.properties
allow for log deletion
. add this line to server.properties
[source, numbered]
---------------------------------------------------------------------
delete.topic.enable=true
---------------------------------------------------------------------
. now bounce kafka-server and zookeeper on each node
[source, numbered]
---------------------------------------------------------------------
$ sudo /etc/init.d/kafka-server stop

$ sudo /etc/init.d/zookeeper stop

$ sudo /etc/init.d/kafka-server start

$ sudo /etc/init.d/zookeeper start
---------------------------------------------------------------------
. now let's list the topic to confirm deletion
[source, numbered]
---------------------------------------------------------------------
vagrant@vagrant-ubuntu-trusty-64:/var/log/kafka$ /usr/bin/kafka-topics --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --list
__confluent.support.metrics
__consumer_offsets
_schemas
greet
test
---------------------------------------------------------------------

== Error on kafka-console-consumer
If this error is encountered, verify that zookeeper is indeed running on the nodes;
[source, numbered]
---------------------------------------------------------------------
vagrant@vagrant-ubuntu-trusty-64:/var/log/kafka$ /usr/bin/kafka-console-consumer --zookeeper 192.168.33.21:2181,192.168.33.22:2181,192.168.33.23:2181 --topic here --from-beginning
[2016-10-05 18:20:52,323] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
test
ing
this thing
may be cool
but not sure
hi from node 1 targeting 21
hi from node 2 targeting 21
hi from node 3 targeting 21
hi from node 2 targeting 22
hi from node 2 targeting 22
hi from node 2 targeting 23
---------------------------------------------------------------------

All the above adapted from https://objectpartners.com/2014/05/06/setting-up-your-own-apache-kafka-cluster-with-vagrant-step-by-step/